{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiantianWang-Sara/Machine-Learning-Projects/blob/main/553_2024_Project_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due Date: December 11th"
      ],
      "metadata": {
        "id": "F3ZF3oYIJ3KW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzptQhdh0pUr"
      },
      "source": [
        "# Vaccine Development with Dynamic Programming\n",
        "\n",
        "You are the CEO of a biotech company which is considering the development of a new vaccine. Starting at phase 0 (state 0), the drug develpment can stay in the same state or advance to \"phase 1  with promising results\" (state 1) or advance to \"phase 1 with disappointing results\" (state 2), or fail completely (state 4). At phase 1, the drug can stay in the same state, fail or become a success (state 3), in which case you will sell its patent to a big pharma company for \\$10 million.\n",
        "These state transitions happen from month to month, and at each state, you have the option to make an additional investment of \\$100,000, which increases the chances of success.\n",
        "\n",
        "After careful study, your analysts develop the program below to simulate different scenarios using statistical data from similar projects.\n",
        "\n",
        "Use a discount factor of 0.996.\n",
        "\n",
        "-  Write a policy iteration or value iteration code to compute the value of this project. Please print the full V (value function) vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnAvrShs6ecs"
      },
      "source": [
        "import numpy as np\n",
        "class MDP():\n",
        "  def __init__(self):\n",
        "    self.A = [0, 1]\n",
        "    self.S = [0, 1, 2, 3, 4]\n",
        "\n",
        "    P0 = np.array([[0.5, .15, .15, 0, .20],\n",
        "                   [0, .5, .0, .25, .25],\n",
        "                   [0, 0, .15, .05, .8],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R0 = np.array([0, 0, 0, 10, 0])\n",
        "\n",
        "    P1 = np.array([[0.5, .25, .15, 0, .10],\n",
        "                   [0, .5, .0, .35, .15],\n",
        "                   [0, 0, .20, .05, .75],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
        "\n",
        "    self.P = [P0, P1]\n",
        "    self.R = [R0, R1]\n",
        "\n",
        "  def step(self, s, a):\n",
        "    s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
        "    R = self.R[a][s]\n",
        "    if s_prime == 4:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "    return s_prime, R, done\n",
        "\n",
        "  def simulate(self, s, a, π):\n",
        "    done = False\n",
        "    t = 0\n",
        "    history = []\n",
        "    while not done:\n",
        "      if t > 0:\n",
        "        a = π[s]\n",
        "      s_prime, R, done = self.step(s, a)\n",
        "      history.append((s, a, R))\n",
        "      s = s_prime\n",
        "      t += 1\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access the transition probability matrices and the reward vector as follows:"
      ],
      "metadata": {
        "id": "xgAiJxSnZtkH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-rfjh_37kmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c35512-458e-4864-ff20-ebca85b266d2"
      },
      "source": [
        "mdp = MDP()\n",
        "P = mdp.P\n",
        "R = mdp.R\n",
        "\n",
        "\n",
        "s = 2 # current state\n",
        "s_prime = 4  # next state\n",
        "a = 1  # chosen action\n",
        "\n",
        "# Probability of transition from state s (2) to s_prime (4) if action == a (1):\n",
        "print(P[a][s, s_prime])\n",
        "\n",
        "# Reward at state s if action = a\n",
        "print(R[a][s])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.75\n",
            "-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWfK-47V8I08"
      },
      "source": [
        "γ = 0.996\n",
        "A = mdp.A\n",
        "S = mdp.S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_Rπ(R, π, S):\n",
        "  Rπ = np.zeros(len(S))\n",
        "  for s in S:\n",
        "    Rπ[s] = R[π[s]][s]\n",
        "  return Rπ\n",
        "\n",
        "\n",
        "def construct_Pπ(P, π, S):\n",
        "  Pπ = np.zeros((len(S), len(S)))\n",
        "  for s in S:\n",
        "    for s_prime in S:\n",
        "      Pπ[s, s_prime] = P[π[s]][s, s_prime]\n",
        "  return Pπ"
      ],
      "metadata": {
        "id": "7Q9WgyVq1f0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(π, Vπ):\n",
        "\n",
        "  # Policy evaluation\n",
        "  Rπ = construct_Rπ(R, π, S)\n",
        "  Pπ = construct_Pπ(P, π, S)\n",
        "  for i in range(1):\n",
        "    Vπ = Rπ + γ * Pπ @ Vπ\n",
        "\n",
        "  # Policy improvement\n",
        "  Qπ = np.zeros((5, 2))\n",
        "\n",
        "  for s in S:\n",
        "    for a in A:\n",
        "      Qπ[s, a] = R[a][s] + γ * P[a][s] @ Vπ\n",
        "\n",
        "  π = np.argmax(Qπ, axis=1)\n",
        "  return π, Vπ"
      ],
      "metadata": {
        "id": "h0CKpC44JWb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "π = [0, 0, 0, 0, 0]\n",
        "Vπ = np.zeros(5)\n",
        "for iteration in range(1000):\n",
        "  π, Vπ = policy_iteration(π, Vπ)\n",
        "\n",
        "print(π)\n",
        "print(Vπ) #the units are in millions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtjheH1vJ-V1",
        "outputId": "efd2a117-fd76-4996-b195-15bf541a8e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 0 0]\n",
            "[ 3.32067538  6.74501992  0.58546908 10.          0.        ]\n"
          ]
        }
      ]
    }
  ]
}